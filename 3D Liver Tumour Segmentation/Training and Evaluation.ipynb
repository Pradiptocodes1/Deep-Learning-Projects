{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37d396e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in c:\\users\\kiit\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (4.23.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa808ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==3.20.0\n",
      "  Downloading protobuf-3.20.0-cp38-cp38-win_amd64.whl (904 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.2\n",
      "    Uninstalling protobuf-4.23.2:\n",
      "      Successfully uninstalled protobuf-4.23.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\KIIT\\\\anaconda3\\\\envs\\\\pytorchenv\\\\Lib\\\\site-packages\\\\google\\\\~upb\\\\_message.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install protobuf==3.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64451448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torchio as tio\n",
    "import torch\n",
    "from model import UNet\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a0989f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_img_to_label_path(path):\n",
    "    \n",
    "    parts = list(path.parts)\n",
    "    parts[parts.index(\"imagesTr\")] = \"labelsTr\"\n",
    "    return Path(*parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0682fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"Task03_Liver_rs/imagesTr/\")\n",
    "subjects_paths = list(path.glob(\"liver_*\"))\n",
    "subjects = []\n",
    "\n",
    "for subject_path in subjects_paths:\n",
    "    label_path = change_img_to_label_path(subject_path)\n",
    "    subject = tio.Subject({\"CT\":tio.ScalarImage(subject_path), \"Label\":tio.LabelMap(label_path)})\n",
    "    subjects.append(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba5430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    assert subject[\"CT\"].orientation==(\"R\",\"A\",\"S\") #RIGHT ANTERIOR SUPERIOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f38b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "process=tio.Compose([\n",
    "    tio.CropOrPad((256,256,200)),\n",
    "    tio.RescaleIntensity((-1,-1))\n",
    "])\n",
    "augmentation=tio.RandomAffine(scales=(0.9,1.1),degrees=(-10,10))\n",
    "val_transform=process\n",
    "train_transform=tio.Compose([process,augmentation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fdd2e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=tio.SubjectsDataset(subjects[:105],transform=train_transform)\n",
    "val_dataset=tio.SubjectsDataset(subjects[:105],transform=val_transform)\n",
    "\n",
    "sampler=tio.data.LabelSampler(patch_size=96,label_name=\"Label\",label_probabilities={0:0.2,1:0.3,2:0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df97bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patches_queue=tio.Queue(\n",
    "train_dataset,\n",
    "max_length=40,\n",
    "samples_per_volume=5,\n",
    "sampler=sampler,\n",
    "num_workers=4)\n",
    "\n",
    "val_patches_queue=tio.Queue(\n",
    "val_dataset,\n",
    "max_length=40,\n",
    "samples_per_volume=5,\n",
    "sampler=sampler,\n",
    "num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6438e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(train_patches_queue,batch_size=2,num_workers=0)\n",
    "val_loader=torch.utils.data.DataLoader(val_patches_queue,batch_size=2,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0758b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = UNet()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, data):\n",
    "        pred = self.model(data)\n",
    "        return pred\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # You can obtain the raw volume arrays by accessing the data attribute of the subject\n",
    "        img = batch[\"CT\"][\"data\"]\n",
    "        mask = batch[\"Label\"][\"data\"][:,0]  # Remove single channel as CrossEntropyLoss expects NxHxW\n",
    "        mask = mask.long()\n",
    "        \n",
    "        pred = self(img)\n",
    "        loss = self.loss_fn(pred, mask)\n",
    "        \n",
    "        # Logs\n",
    "        self.log(\"Train Loss\", loss)\n",
    "        if batch_idx % 50 == 0:\n",
    "            self.log_images(img.cpu(), pred.cpu(), mask.cpu(), \"Train\")\n",
    "        return loss\n",
    "    \n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # You can obtain the raw volume arrays by accessing the data attribute of the subject\n",
    "        img = batch[\"CT\"][\"data\"]\n",
    "        mask = batch[\"Label\"][\"data\"][:,0]  # Remove single channel as CrossEntropyLoss expects NxHxW\n",
    "        mask = mask.long()\n",
    "        \n",
    "        pred = self(img)\n",
    "        loss = self.loss_fn(pred, mask)\n",
    "        \n",
    "        # Logs\n",
    "        self.log(\"Val Loss\", loss)\n",
    "        self.log_images(img.cpu(), pred.cpu(), mask.cpu(), \"Val\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def log_images(self, img, pred, mask, name):\n",
    "        \n",
    "        results = []\n",
    "        pred = torch.argmax(pred, 1) # Take the output with the highest value\n",
    "        axial_slice = 50  # Always plot slice 50 of the 96 slices\n",
    "        \n",
    "        fig, axis = plt.subplots(1, 2)\n",
    "        axis[0].imshow(img[0][0][:,:,axial_slice], cmap=\"bone\")\n",
    "        mask_ = np.ma.masked_where(mask[0][:,:,axial_slice]==0, mask[0][:,:,axial_slice])\n",
    "        axis[0].imshow(mask_, alpha=0.6)\n",
    "        axis[0].set_title(\"Ground Truth\")\n",
    "        \n",
    "        axis[1].imshow(img[0][0][:,:,axial_slice], cmap=\"bone\")\n",
    "        mask_ = np.ma.masked_where(pred[0][:,:,axial_slice]==0, pred[0][:,:,axial_slice])\n",
    "        axis[1].imshow(mask_, alpha=0.6, cmap=\"autumn\")\n",
    "        axis[1].set_title(\"Pred\")\n",
    "\n",
    "        self.logger.experiment.add_figure(f\"{name} Prediction vs Label\", fig, self.global_step)\n",
    "\n",
    "            \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        return [self.optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bad3d432",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Segmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2604a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='Val Loss',\n",
    "    save_top_k=10,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4df48291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(gpus=0,logger=TensorBoardLogger(save_dir=\"./logs\"),log_every_n_steps=1,\n",
    "                   callbacks=checkpoint_callback, max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccea0c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | model   | UNet             | 5.8 M \n",
      "1 | loss_fn | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "5.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 M     Total params\n",
      "23.344    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02916860580444336,
       "initial": 0,
       "n": 0,
       "ncols": 119,
       "nrows": 29,
       "postfix": null,
       "prefix": "Validation sanity check",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\envs\\pytorchenv\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009010553359985352,
       "initial": -1,
       "n": -1,
       "ncols": 119,
       "nrows": 29,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0371bd8dbdbe4794b1db41e287a3928d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\envs\\pytorchenv\\lib\\site-packages\\matplotlib\\image.py:443: UserWarning: Warning: converting a masked element to nan.\n",
      "  dv = np.float64(self.norm.vmax) - np.float64(self.norm.vmin)\n",
      "C:\\Users\\KIIT\\anaconda3\\envs\\pytorchenv\\lib\\site-packages\\matplotlib\\image.py:444: UserWarning: Warning: converting a masked element to nan.\n",
      "  vmid = np.float64(self.norm.vmin) + dv / 2\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0061168670654296875,
       "initial": 0,
       "n": 0,
       "ncols": 119,
       "nrows": 29,
       "postfix": null,
       "prefix": "Validating",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015649795532226562,
       "initial": 0,
       "n": 0,
       "ncols": 119,
       "nrows": 29,
       "postfix": null,
       "prefix": "Validating",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015512943267822266,
       "initial": 0,
       "n": 0,
       "ncols": 119,
       "nrows": 29,
       "postfix": null,
       "prefix": "Validating",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007100820541381836,
       "initial": 0,
       "n": 0,
       "ncols": 119,
       "nrows": 29,
       "postfix": null,
       "prefix": "Validating",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03098773956298828,
       "initial": 0,
       "n": 0,
       "ncols": 119,
       "nrows": 29,
       "postfix": null,
       "prefix": "Validating",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.026397705078125,
       "initial": 0,
       "n": 0,
       "ncols": 119,
       "nrows": 29,
       "postfix": null,
       "prefix": "Validating",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\envs\\pytorchenv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1051: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model,train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf3eafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from celluloid import Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "905a1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Segmenter.load_from_checkpoint(\"weights/epoch=97-step=25773.ckpt\")\n",
    "model = model.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f79ffedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a validation subject and extract the images and segmentation for evaluation\n",
    "IDX = 4\n",
    "mask = val_dataset[IDX][\"Label\"][\"data\"]\n",
    "imgs = val_dataset[IDX][\"CT\"][\"data\"]\n",
    "\n",
    "# GridSampler\n",
    "grid_sampler = tio.inference.GridSampler(val_dataset[IDX], 96, (8, 8, 8))\n",
    "# GridAggregator\n",
    "aggregator = tio.inference.GridAggregator(grid_sampler)\n",
    "# DataLoader for speed up\n",
    "patch_loader = torch.utils.data.DataLoader(grid_sampler, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c70f5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\envs\\pytorchenv\\lib\\site-packages\\torch\\nn\\functional.py:3609: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "with torch.no_grad():\n",
    "    for patches_batch in patch_loader:\n",
    "        input_tensor = patches_batch['CT'][\"data\"].to(device)  # Get batch of patches\n",
    "        locations = patches_batch[tio.LOCATION]  # Get locations of patches\n",
    "        pred = model(input_tensor)  # Compute prediction\n",
    "        aggregator.add_batch(pred, locations)  # Combine predictions to volume\n",
    "        \n",
    "# Extract the volume prediction\n",
    "output_tensor = aggregator.get_output_tensor()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08c4535b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\envs\\pytorchenv\\lib\\site-packages\\matplotlib\\animation.py:973: UserWarning: Animation was deleted without rendering anything. This is most likely unintended. To prevent deletion, assign the Animation to a variable that exists for as long as you need the Animation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGiCAYAAAC/NyLhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAd+ElEQVR4nO3df2zV1f3H8VcL5QrCvbWU9rbyw4IKYoFtgN2Nk5nRtCWEiPCHYJMhIRCwNSLIXE0EMcu6abIt7svknwVc4lBJRCNRkq6lJcxSpUoU0IaSuqL0trOk9xaQ0tLz/eP75ZNdrUBL27t3+3wkJ+F+Pufee+5J69Pb+6EkOOecAAAwIjHeCwAAoDcIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMCUuIVrx44duuOOO3TLLbcoJydHH374YbyWAgAwJC7heuONN7Rp0yZt27ZNH3/8sebMmaP8/Hy1tLTEYzkAAEMS4vFLdnNycjR//nz9z//8jySpu7tbkyZN0hNPPKFf//rXg70cAIAhIwf7CS9fvqza2lqVlJR4xxITE5Wbm6vq6uoe79PR0aGOjg7vdnd3t86dO6fx48crISFhwNcMAOhfzjm1t7crMzNTiYm9++HfoIfrm2++0ZUrV5Senh5zPD09XV988UWP9yktLdX27dsHY3kAgEF05swZTZw4sVf3MXFVYUlJiSKRiDcaGxvjvSQAQD8YN25cr+8z6O+4UlNTNWLECDU3N8ccb25uVjAY7PE+Pp9PPp9vMJYHABhEffm4Z9DfcY0aNUpz585VeXm5d6y7u1vl5eUKhUKDvRwAgDGD/o5LkjZt2qRVq1Zp3rx5uu+++/SnP/1JFy5c0OrVq+OxHACAIXEJ1yOPPKJ///vf2rp1q8LhsH70ox/pwIED37tgAwCA74rL3+O6WdFoVIFAIN7LAADcpEgkIr/f36v7mLiqEACAqwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAU/o9XM8//7wSEhJixowZM7zzly5dUlFRkcaPH6+xY8dq+fLlam5u7u9lAACGqAF5x3XvvfeqqanJG4cPH/bOPfXUU3r33Xe1d+9eVVVV6ezZs1q2bNlALAMAMASNHJAHHTlSwWDwe8cjkYj++te/6u9//7t+8YtfSJJ27dqle+65R0eOHNFPf/rTgVgOAGAIGZB3XKdOnVJmZqamTp2qwsJCNTY2SpJqa2vV2dmp3Nxcb+6MGTM0efJkVVdXD8RSAABDTL+/48rJydHu3bs1ffp0NTU1afv27XrggQd0/PhxhcNhjRo1SsnJyTH3SU9PVzgc/sHH7OjoUEdHh3c7Go3297IBAEb0e7gWLVrk/Xn27NnKycnRlClT9Oabb2r06NF9eszS0lJt3769v5YIADBswC+HT05O1t133636+noFg0FdvnxZbW1tMXOam5t7/EzsqpKSEkUiEW+cOXNmgFcNAPhvNeDhOn/+vE6fPq2MjAzNnTtXSUlJKi8v987X1dWpsbFRoVDoBx/D5/PJ7/fHDADA8NTvPyp8+umntWTJEk2ZMkVnz57Vtm3bNGLECK1cuVKBQEBr1qzRpk2blJKSIr/fryeeeEKhUIgrCgEAN6Tfw/XVV19p5cqVam1t1YQJE/Szn/1MR44c0YQJEyRJf/zjH5WYmKjly5ero6ND+fn5+stf/tLfywAADFEJzjkX70X0VjQaVSAQiPcyAAA3KRKJ9PrjH35XIQDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAlF6H69ChQ1qyZIkyMzOVkJCgt99+O+a8c05bt25VRkaGRo8erdzcXJ06dSpmzrlz51RYWCi/36/k5GStWbNG58+fv6kXAgAYHnodrgsXLmjOnDnasWNHj+dffPFFvfzyy9q5c6dqamp06623Kj8/X5cuXfLmFBYW6sSJEyorK9P+/ft16NAhrVu3ru+vAgAwfLibIMnt27fPu93d3e2CwaB76aWXvGNtbW3O5/O5PXv2OOecO3nypJPkPvroI2/O+++/7xISEtzXX399Q88biUScJAaDwWAYH5FIpNft6dfPuBoaGhQOh5Wbm+sdCwQCysnJUXV1tSSpurpaycnJmjdvnjcnNzdXiYmJqqmp6c/lAACGoJH9+WDhcFiSlJ6eHnM8PT3dOxcOh5WWlha7iJEjlZKS4s35ro6ODnV0dHi3o9Fofy4bAGCIiasKS0tLFQgEvDFp0qR4LwkAECf9Gq5gMChJam5ujjne3NzsnQsGg2ppaYk539XVpXPnznlzvqukpESRSMQbZ86c6c9lAwAM6ddwZWVlKRgMqry83DsWjUZVU1OjUCgkSQqFQmpra1Ntba03p6KiQt3d3crJyenxcX0+n/x+f8wAAAxPvf6M6/z586qvr/duNzQ06NixY0pJSdHkyZO1ceNG/eY3v9Fdd92lrKwsPffcc8rMzNTSpUslSffcc48KCgq0du1a7dy5U52dnSouLtaKFSuUmZnZby8MADBE9fYyxIMHD/Z4SeOqVaucc/93Sfxzzz3n0tPTnc/ncwsXLnR1dXUxj9Ha2upWrlzpxo4d6/x+v1u9erVrb2+/4TVwOTyDwWAMjdGXy+ETnHNOxkSjUQUCgXgvAwBwkyKRSK8//jFxVSEAAFcRLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKb0O16FDh7RkyRJlZmYqISFBb7/9dsz5xx57TAkJCTGjoKAgZs65c+dUWFgov9+v5ORkrVmzRufPn7+pFwIAGB56Ha4LFy5ozpw52rFjxw/OKSgoUFNTkzf27NkTc76wsFAnTpxQWVmZ9u/fr0OHDmndunW9Xz0AYPhxN0GS27dvX8yxVatWuYceeugH73Py5EknyX300Ufesffff98lJCS4r7/++oaeNxKJOEkMBoPBMD4ikUiv2zMgn3FVVlYqLS1N06dP14YNG9Ta2uqdq66uVnJysubNm+cdy83NVWJiompqanp8vI6ODkWj0ZgBABie+j1cBQUF+tvf/qby8nL9/ve/V1VVlRYtWqQrV65IksLhsNLS0mLuM3LkSKWkpCgcDvf4mKWlpQoEAt6YNGlSfy8bAGDEyP5+wBUrVnh/njVrlmbPnq1p06apsrJSCxcu7NNjlpSUaNOmTd7taDRKvABgmBrwy+GnTp2q1NRU1dfXS5KCwaBaWlpi5nR1dencuXMKBoM9PobP55Pf748ZAIDhacDD9dVXX6m1tVUZGRmSpFAopLa2NtXW1npzKioq1N3drZycnIFeDgDAuF7/qPD8+fPeuydJamho0LFjx5SSkqKUlBRt375dy5cvVzAY1OnTp/WrX/1Kd955p/Lz8yVJ99xzjwoKCrR27Vrt3LlTnZ2dKi4u1ooVK5SZmdl/rwwAMDT19jLEgwcP9nhJ46pVq9zFixddXl6emzBhgktKSnJTpkxxa9eudeFwOOYxWltb3cqVK93YsWOd3+93q1evdu3t7Te8Bi6HZzAYjKEx+nI5fIJzzsmYaDSqQCAQ72UAAG5SJBLp9XUL/K5CAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIApvQpXaWmp5s+fr3HjxiktLU1Lly5VXV1dzJxLly6pqKhI48eP19ixY7V8+XI1NzfHzGlsbNTixYs1ZswYpaWlacuWLerq6rr5VwMAGPJ6Fa6qqioVFRXpyJEjKisrU2dnp/Ly8nThwgVvzlNPPaV3331Xe/fuVVVVlc6ePatly5Z5569cuaLFixfr8uXL+uCDD/Tqq69q9+7d2rp1a/+9KgDA0OVuQktLi5PkqqqqnHPOtbW1uaSkJLd3715vzueff+4kuerqauecc++9955LTEx04XDYm/PKK684v9/vOjo6buh5I5GIk8RgMBgM4yMSifS6PTf1GVckEpEkpaSkSJJqa2vV2dmp3Nxcb86MGTM0efJkVVdXS5Kqq6s1a9Yspaene3Py8/MVjUZ14sSJHp+no6ND0Wg0ZgAAhqc+h6u7u1sbN27U/fffr+zsbElSOBzWqFGjlJycHDM3PT1d4XDYm/Of0bp6/uq5npSWlioQCHhj0qRJfV02AMC4PoerqKhIx48f1+uvv96f6+lRSUmJIpGIN86cOTPgzwkA+O80si93Ki4u1v79+3Xo0CFNnDjROx4MBnX58mW1tbXFvOtqbm5WMBj05nz44Ycxj3f1qsOrc77L5/PJ5/P1ZakAgCGmV++4nHMqLi7Wvn37VFFRoaysrJjzc+fOVVJSksrLy71jdXV1amxsVCgUkiSFQiF99tlnamlp8eaUlZXJ7/dr5syZN/NaAADDQW+u5NiwYYMLBAKusrLSNTU1eePixYvenPXr17vJkye7iooKd/ToURcKhVwoFPLOd3V1uezsbJeXl+eOHTvmDhw44CZMmOBKSkpueB1cVchgMBhDY/TlqsJeheuHnnjXrl3enG+//dY9/vjj7rbbbnNjxoxxDz/8sGtqaop5nC+//NItWrTIjR492qWmprrNmze7zs7OG14H4WIwGIyhMfoSroT/D5Ip0WhUgUAg3ssAANykSCQiv9/fq/vwuwoBAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKb0KlylpaWaP3++xo0bp7S0NC1dulR1dXUxcx588EElJCTEjPXr18fMaWxs1OLFizVmzBilpaVpy5Yt6urquvlXAwAY8kb2ZnJVVZWKioo0f/58dXV16dlnn1VeXp5OnjypW2+91Zu3du1avfDCC97tMWPGeH++cuWKFi9erGAwqA8++EBNTU365S9/qaSkJP32t7/th5cEABjS3E1oaWlxklxVVZV37Oc//7l78sknf/A+7733nktMTHThcNg79sorrzi/3+86Ojpu6HkjkYiTxGAwGAzjIxKJ9Lo9N/UZVyQSkSSlpKTEHH/ttdeUmpqq7OxslZSU6OLFi9656upqzZo1S+np6d6x/Px8RaNRnThxosfn6ejoUDQajRkAgOGpVz8q/E/d3d3auHGj7r//fmVnZ3vHH330UU2ZMkWZmZn69NNP9cwzz6iurk5vvfWWJCkcDsdES5J3OxwO9/hcpaWl2r59e1+XCgAYQvocrqKiIh0/flyHDx+OOb5u3Trvz7NmzVJGRoYWLlyo06dPa9q0aX16rpKSEm3atMm7HY1GNWnSpL4tHABgWp9+VFhcXKz9+/fr4MGDmjhx4jXn5uTkSJLq6+slScFgUM3NzTFzrt4OBoM9PobP55Pf748ZAIDhqVfhcs6puLhY+/btU0VFhbKysq57n2PHjkmSMjIyJEmhUEifffaZWlpavDllZWXy+/2aOXNmb5YDABiOenMlx4YNG1wgEHCVlZWuqanJGxcvXnTOOVdfX+9eeOEFd/ToUdfQ0ODeeecdN3XqVLdgwQLvMbq6ulx2drbLy8tzx44dcwcOHHATJkxwJSUlN7wOripkMBiMoTH6clVhr8L1Q0+8a9cu55xzjY2NbsGCBS4lJcX5fD535513ui1btnxvYV9++aVbtGiRGz16tEtNTXWbN292nZ2dN7wOwsVgMBhDY/QlXAn/HyRTotGoAoFAvJcBALhJkUik19ctmPxdhQZbCwDoQV/+e24yXO3t7fFeAgCgH/Tlv+cmf1TY3d2turo6zZw5U2fOnOHy+B5c/btu7E/P2J9rY3+ujz26tuvtj3NO7e3tyszMVGJi795D9fkvIMdTYmKibr/9dkni73VdB/tzbezPtbE/18ceXdu19qev1yqY/FEhAGD4IlwAAFPMhsvn82nbtm3y+XzxXsp/Jfbn2tifa2N/ro89uraB3B+TF2cAAIYvs++4AADDE+ECAJhCuAAAphAuAIApJsO1Y8cO3XHHHbrllluUk5OjDz/8MN5Liovnn39eCQkJMWPGjBne+UuXLqmoqEjjx4/X2LFjtXz58u/9I55DzaFDh7RkyRJlZmYqISFBb7/9dsx555y2bt2qjIwMjR49Wrm5uTp16lTMnHPnzqmwsFB+v1/Jyclas2aNzp8/P4ivYuBcb38ee+yx731NFRQUxMwZqvtTWlqq+fPna9y4cUpLS9PSpUtVV1cXM+dGvqcaGxu1ePFijRkzRmlpadqyZYu6uroG86UMmBvZowcffPB7X0Pr16+PmXOze2QuXG+88YY2bdqkbdu26eOPP9acOXOUn58f8w9TDif33nuvmpqavHH48GHv3FNPPaV3331Xe/fuVVVVlc6ePatly5bFcbUD78KFC5ozZ4527NjR4/kXX3xRL7/8snbu3Kmamhrdeuutys/P16VLl7w5hYWFOnHihMrKyrR//34dOnRI69atG6yXMKCutz+SVFBQEPM1tWfPnpjzQ3V/qqqqVFRUpCNHjqisrEydnZ3Ky8vThQsXvDnX+566cuWKFi9erMuXL+uDDz7Qq6++qt27d2vr1q3xeEn97kb2SJLWrl0b8zX04osveuf6ZY96/Q+hxNl9993nioqKvNtXrlxxmZmZrrS0NI6rio9t27a5OXPm9Hiura3NJSUlub1793rHPv/8cyfJVVdXD9IK40uS27dvn3e7u7vbBYNB99JLL3nH2tranM/nc3v27HHOOXfy5EknyX300UfenPfff98lJCS4r7/+etDWPhi+uz/OObdq1Sr30EMP/eB9htP+tLS0OEmuqqrKOXdj31PvvfeeS0xMdOFw2JvzyiuvOL/f7zo6Ogb3BQyC7+6Rc879/Oc/d08++eQP3qc/9sjUO67Lly+rtrZWubm53rHExETl5uaquro6jiuLn1OnTikzM1NTp05VYWGhGhsbJUm1tbXq7OyM2asZM2Zo8uTJw3avGhoaFA6HY/YkEAgoJyfH25Pq6molJydr3rx53pzc3FwlJiaqpqZm0NccD5WVlUpLS9P06dO1YcMGtba2eueG0/5EIhFJUkpKiqQb+56qrq7WrFmzlJ6e7s3Jz89XNBrViRMnBnH1g+O7e3TVa6+9ptTUVGVnZ6ukpEQXL170zvXHHpn6JbvffPONrly5EvOCJSk9PV1ffPFFnFYVPzk5Odq9e7emT5+upqYmbd++XQ888ICOHz+ucDisUaNGKTk5OeY+6enpCofD8VlwnF193T19/Vw9Fw6HlZaWFnN+5MiRSklJGRb7VlBQoGXLlikrK0unT5/Ws88+q0WLFqm6ulojRowYNvvT3d2tjRs36v7771d2drYk3dD3VDgc7vHr6+q5oaSnPZKkRx99VFOmTFFmZqY+/fRTPfPMM6qrq9Nbb70lqX/2yFS4EGvRokXen2fPnq2cnBxNmTJFb775pkaPHh3HlcGqFStWeH+eNWuWZs+erWnTpqmyslILFy6M48oGV1FRkY4fPx7zmTFi/dAe/efnnbNmzVJGRoYWLlyo06dPa9q0af3y3KZ+VJiamqoRI0Z87yqe5uZmBYPBOK3qv0dycrLuvvtu1dfXKxgM6vLly2pra4uZM5z36urrvtbXTzAY/N6FPl1dXTp37tyw3LepU6cqNTVV9fX1kobH/hQXF2v//v06ePCgJk6c6B2/ke+pYDDY49fX1XNDxQ/tUU9ycnIkKeZr6Gb3yFS4Ro0apblz56q8vNw71t3drfLycoVCoTiu7L/D+fPndfr0aWVkZGju3LlKSkqK2au6ujo1NjYO273KyspSMBiM2ZNoNKqamhpvT0KhkNra2lRbW+vNqaioUHd3t/cNOJx89dVXam1tVUZGhqShvT/OORUXF2vfvn2qqKhQVlZWzPkb+Z4KhUL67LPPYuJeVlYmv9+vmTNnDs4LGUDX26OeHDt2TJJivoZueo/6eDFJ3Lz++uvO5/O53bt3u5MnT7p169a55OTkmCtUhovNmze7yspK19DQ4P75z3+63Nxcl5qa6lpaWpxzzq1fv95NnjzZVVRUuKNHj7pQKORCoVCcVz2w2tvb3SeffOI++eQTJ8n94Q9/cJ988on717/+5Zxz7ne/+51LTk5277zzjvv000/dQw895LKysty3337rPUZBQYH78Y9/7Gpqatzhw4fdXXfd5VauXBmvl9SvrrU/7e3t7umnn3bV1dWuoaHB/eMf/3A/+clP3F133eUuXbrkPcZQ3Z8NGza4QCDgKisrXVNTkzcuXrzozbne91RXV5fLzs52eXl57tixY+7AgQNuwoQJrqSkJB4vqd9db4/q6+vdCy+84I4ePeoaGhrcO++846ZOneoWLFjgPUZ/7JG5cDnn3J///Gc3efJkN2rUKHffffe5I0eOxHtJcfHII4+4jIwMN2rUKHf77be7Rx55xNXX13vnv/32W/f444+72267zY0ZM8Y9/PDDrqmpKY4rHngHDx50kr43Vq1a5Zz7v0vin3vuOZeenu58Pp9buHChq6uri3mM1tZWt3LlSjd27Fjn9/vd6tWrXXt7exxeTf+71v5cvHjR5eXluQkTJrikpCQ3ZcoUt3bt2u/9T+FQ3Z+e9kWS27VrlzfnRr6nvvzyS7do0SI3evRol5qa6jZv3uw6OzsH+dUMjOvtUWNjo1uwYIFLSUlxPp/P3XnnnW7Lli0uEonEPM7N7hH/rAkAwBRTn3EBAEC4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGDK/wJ4QN0EeqBKGgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "camera = Camera(fig)  # create the camera object from celluloid\n",
    "pred = output_tensor.argmax(0)\n",
    "\n",
    "for i in range(0, output_tensor.shape[3], 2):  # axial view\n",
    "    plt.imshow(imgs[0,:,:,i], cmap=\"bone\")\n",
    "    mask_ = np.ma.masked_where(pred[:,:,i]==0, pred[:,:,i])\n",
    "    label_mask = np.ma.masked_where(mask[0,:,:,i]==0, mask[0,:,:,i])\n",
    "    plt.imshow(mask_, alpha=0.1, cmap=\"autumn\")\n",
    "    #plt.imshow(label_mask, alpha=0.5, cmap=\"jet\")  # Uncomment if you want to see the label\n",
    "\n",
    "    # plt.axis(\"off\")\n",
    "    camera.snap()  # Store the current slice\n",
    "animation = camera.animate()  # create the animation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e4e6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ad11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
